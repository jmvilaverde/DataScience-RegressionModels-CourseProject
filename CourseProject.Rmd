---
title: "Motor Trend magazine  - Data analysis of influence on MPG for Automatic vs. Manual Transmission."
author: "by jmvilaverde"
date: "Thursday, June 18, 2015"
output: pdf_document
---

###Executive summary (First paragraph)

For all models evaluated that have a P-value Manual transmission is better for MPG...


#### Data adquisition

##### 1. Data adquisition and Initial structure analysis

##### 1.1. Variables


```{r dataAdquisition, echo=TRUE}
data(mtcars)

#Show colnames of mtcars
names(mtcars)
```

**?mtcars**

Format: _A data frame with 32 observations on 11 variables._

Variable| Units                                         | Values
------- | --------------------------------------------- | ---------------------------
**mpg** | **Miles/(US) gallon**                         |
cyl     | Number of cylinders (4,6,8)                   | `r unique(mtcars$cyl)[order(unique(mtcars$cyl))]`
disp    | Displacement (cu.in.)                         |
hp      | Gross horsepower                              |
drat    | Rear axle ratio                               |
wt      | Weight (lb/1000)                              |
qsec    | 1/4 mile time                                 |
vs      | V/S -> V motor or straight motor              | `r unique(mtcars$vs)[order(unique(mtcars$vs))]`
**am**  | **Transmission (0 = automatic, 1 = manual)**  | `r unique(mtcars$am)[order(unique(mtcars$am))]`
gear    | Number of forward gears                       | `r unique(mtcars$gear)[order(unique(mtcars$gear))]`
carb    | Number of carburetors                         | `r unique(mtcars$carb)[order(unique(mtcars$carb))]`

```{r structureMtcars, echo=TRUE}
#Show structure of mtcars
str(mtcars)
```


### 2.Initial analysis

#### One predictor

Selected variables: 

* Predictor:    X = am,  Transmission with values 0 for automatic, 1 for manual.
* Outcome:      Y = mpg, Miles/(US) gallon

Linear regression model formula: Y = ${\beta}_0 + {\beta}_1X$ -> mpg = ${\beta}_0 + {\beta}_1am$

Create initial model:

* Calculate $\beta_0$ (Intercept): $\hat{\beta}_0=\bar{Y}+\hat{\beta}_1 \bar{X}$
* Calculate $\beta_1$ (Slope): $\hat{\beta}_1= Cor(Y,X)\frac{Sd(Y)}{Sd(X)}$

Fitting the best line: $\Sigma_{n}^{i=1}{(Y_i-(\beta_0+\beta_1X_i))}^2$ to minimize the distance.

```{r teoreticalFormula, echo=TRUE}
#Theoretical formula
y <- mtcars$mpg
x <- mtcars$am
beta1 <- cor(y,x)*(sd(y)/sd(x))
beta0 <- mean(y) - beta1 * mean(x)

#R function
model.Initial <- lm(mpg ~ am, data=mtcars)
coeffs <- coef(model.Initial)
summary(model.Initial)
#Comparation
rbind(c(beta0, beta1), coeffs)
```

For linear model $Y=\beta_0+\beta_1X$ -> $mpg=\beta_0+\beta_1am$

* Intercept $\beta_0=$ `r beta0`
* Slope $\beta_1=$ `r beta1`

> With this linear model we have a P-value under 0.05 on Intercept and all the coefficents and overall model.
> $R^2$ is 0.3598 for the model, this low value indicates that the model is not a good fit to the data.

> For this linear regression model, the expected difference in MPG is `r beta1` when the car have manual transmission in comparation to the same car with automatic transmission.
The Intercept `r beta0` is the expected MPG of a automatic transmission car.

#### Multiple predictor

In multivariable regression analysis you must evaluate the consecuences to throwing variables that aren't related to the outcome and consecuences to omitting variables that are related to the outcome.

Multivariable linear model formula:

* $Y_i=\beta_1X_{1i}+\beta_2X_{2i}+...+\beta_pX_{pi}+\epsilon_i=\Sigma^p_{k=1}X_{ik}\beta_j+\epsilon_i$

Model Complete: $mpg =\beta_{cyl}cyl+\beta_{dips}disp+\beta_{hp}hp+\beta_{drat}drat+\beta_{wt}wt+\beta_{qsec}qsec+\beta_{vs}vs+\beta_{am}am+\beta_{gear}gear+\beta_{carb}carb$

```{r modelComplete, echo=TRUE}
model.Complete <- lm(mpg ~ ., data=mtcars)
beta.am.Complete <- coef(model.Complete)["am"]
summary(model.Complete)
```

> With this linear model we have a P-value over 0.05 on Intercept and all the coefficents, and under 0.05 for overall model.
> $R^2$ is 0.869 for the model, this high value indicates that the model is a good fit to the data.

> For linear regression model _Complete_, the expected difference in MPG is `r beta.am.Complete` when the car have manual transmission in comparation to the same car with automatic transmission.

```{r interactionsModelComplete, echo=TRUE}
#Confidence Interval
sumCoef.Complete <- summary(model.Complete)$coef
confInterval.Model.Complete <- sumCoef.Complete[9,1] + c(-1,1) * qt(0.975, df=model.Complete$df) * sumCoef.Complete[9,2]
```

Model to be proposed. Define a model based on Variation Inflation Factor analysis to check collinearity between variables:

```{r vifAnalysis, echo=TRUE}
library(car)
vif.Complete <- vif(model.Complete)
vif.Complete
```

VIF       | Indication
--------- | ----------
1         | No correlation
$1<VIF<5$ | moderate correlation
$VIF>5$   | strong correlation

Remove variables with strong correlation: `r colnames(vif.Complete[vif.Complete>5])`
Use the other variables: `r colnames(vif.Complete[vif.Complete<5])`

Proposed model: $mpg =\beta_{prop.hp}hp+\beta_{prop.drat}drat+\beta_{prop.vs}vs+\beta_{prop.am}am$

```{r modelProp, echo=TRUE}
model.Prop <- lm(formula = mpg ~ drat + vs + am, data = mtcars)
beta.am.Prop <- coef(model.Prop)["am"]
vif.Prop <- vif(model.Prop)
sqrt(vif.Prop)
summary(model.Prop)
```
Bad summary
> For linear regression model _Prop_, the expected difference in MPG is `r beta.am.Prop` when the car have manual transmission in comparation to the same car with automatic transmission.

Obtain a model by R function step:
```{r modelStep, echo=TRUE}
step(model.Complete, trace=FALSE)
model.Step <- lm(formula = mpg ~ wt + qsec + am, data = mtcars)
step(model.Complete, direction="forward", trace=FALSE)
summary(model.Step)
```

Do a comparation between the 3 models

```{r comparationBetweenModels, echo=TRUE}
library(car)
anova(model.Initial, model.Prop, model.Step, model.Complete)
```








_View Figure 2 for plot with regression line._

##### 3.Basic regression model with additive Gaussian errors.

Into point 2 is obtained a linear regression model that fits the data, but doesn't take in consideration the impact of the others variables. We are going to analyze gaussian errors for Initial Model.

Selected variables: 

* **Predictor**:    X = am,  Transmission with values 0 for automatic, 1 for manual.
* **Outcome**:      Y = mpg, Miles/(US) gallon

Probabilistic model for linear regression:

* $Y_i={\beta}_0+{\beta}_1X_i+\epsilon_i$ -> $mpg_i={\beta}_0+{\beta}_1am_i+\epsilon_i$
* $\epsilon_i$ are assumed iid $N(\mu_i,\sigma^2)$. 
* Note, $E[Y_i|X_i=x_i]=\mu_i=\beta_0+\beta_1x_i$ and $Var(Y_i|X_i=x_i)=\sigma^2$.
* $\hat{\beta}_0=\bar{Y}+\hat{\beta}_1 \bar{X}$ and $\hat{\beta}_1= Cor(Y,X)\frac{Sd(Y)}{Sd(X)}$.

Residuals analysis:

* Observed outcome i is $Y_i$ at a predictor value $X_i$.
* Predicted outcome i is $\hat{Y}_i$ at a predictor value $X_i$ is $\hat{Y}_i=\hat{\beta}_0+\hat{\beta}_1X_i$.
* Residual is the difference between observed an predicted: $e_i=Y_i-\hat{Y}$, the vertical distance between the observed data point and the regression line.
* Least squares minimizes $\Sigma_{i=1}^ne_i^2$.
* $e_i$ can be thought of as estimates of the $\epsilon_i$.
```{r residualsModelInitial, echo=TRUE}
#Calculate residuals
e.ModelInitial <- y-(beta0+beta1*x)
#R function for residuals: resid(model.Initial)

#Calculate predicted y (mpg)
yhat <- predict(model.Initial)
#Calculate max difference between residual and observed Y - predicted Y (y hat)
max(abs(e.ModelInitial -(y - yhat)))
```

```{r residualsModelComplete, echo=TRUE}
#Calculate residuals
e.ModelComplete <- resid(model.Complete)

#Calculate predicted y (mpg)
yhat <- predict(model.Complete)
#Calculate max difference between residual and observed Y - predicted Y (y hat)
max(abs(e.ModelComplete -(y - yhat)))
```

```{r residualsModelStep, echo=TRUE}
#Calculate residuals
e.Model.Step <- resid(model.Step)

#Calculate predicted y (mpg)
yhat <- predict(model.Step)
#Calculate max difference between residual and observed Y - predicted Y (y hat)
max(abs(e.Model.Step -(y - yhat)))
```

Figure 3 Plot of residuals:

Formula to estimate residual variation: 

* ML estimate of $\sigma^2$ is $\frac{1}{n}\Sigma^n_{i=1}e^2_i$
* For $E[\hat\sigma^2]=\sigma^2$ most people use $\frac{1}{n-2}\Sigma^n_{i=1}e^2_i$

```{r residualVariationModelInitial, echo=TRUE}
#Calculate variation
n <- length(y)
var.e.Model.Initial <- sqrt((1/(n-2))*sum((e.ModelInitial^2)))
var.e.Model.Initial
#R function to calculate residual variation
summary(model.Initial)$sigma
```

Model Initial has a residual variation of `r var.e.Model.Initial`.

#### Total variation.

Formula Total variation = Residual variation + Regression Variation:

* $\Sigma^n_{i=1}(Y_i-\bar{Y})^2=\Sigma^n_{i=1}(Y_i-\hat{Y}_i)^2+\Sigma^n_{i=1}(\hat{Y}_i-\bar{Y})^2$

Define the percent of total variation described by the model as:

* $R^2=\frac{\Sigma^n_{i=1}(\hat{Y}_i-\bar{Y})^2}{\Sigma^n_{i=1}(Y_i-\bar{Y})^2}=1-\frac{\Sigma^n_{i=1}(Y_i-\hat{Y}_i)^2}{\Sigma^n_{i=1}(Y_i-\bar{Y})^2}$

Relation between $R^2$ and r (the correlation):

Recall that: $(\hat{Y}_i-\bar{Y})=\hat\beta_1(X_i-\bar{X})$ so that

* $R^2=\frac{\Sigma^n_{i=1}(\hat{Y}_i-\bar{Y})^2}{\Sigma^n_{i=1}(Y_i-\bar{Y})^2}=\hat\beta^2_1\frac{\Sigma^n_{i=1}(X_i-\bar{X})}{\Sigma^n_{i=1}(Y_i-\bar{Y})^2}=Cor(Y,X)^2$


```{r rModelInitial, echo=TRUE}
#Calculate R2
R2.Model.Initial <- sum((yhat - mean(y))^2)/sum((y-mean(y))^2)
R2.Model.Initial
#R function for R2
cor(y,x)^2
```

Inference in regression

Create confidence intervals and perform hypothesis tests.

```{r confIntervalsModelInitial,echo=TRUE}
#Calculation of coefficients
sigma <- var.e.Model.Initial
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1/n + mean(x)^2/ssx) ^ 0.5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0
tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df=n-2, lower.tail=FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df=n-2, lower.tail=FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate","Std.Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable

#R function for calculate coefficients
summary(model.Initial)$coef
```

##Final Analysis

### Is an automatic or manual transmission better for MPG?

For model Initial, model Complete and model Step:

> The manual transmission is better for MPG.

### Quantify the MPG difference between automatic and manual transmissions.

Model Initial:

```{r confidenceIntervalModelInitial, echo=TRUE}
sumCoef <- summary(model.Initial)$coef
confInterval.Model.Initial <- sumCoef[1,1] + c(-1,1) * qt(0.975, df=model.Initial$df) * sumCoef[1,2]
```

> With 95% confidence, we estimate that a manual transmission results in a `r confInterval.Model.Initial[1]` to `r confInterval.Model.Initial[2]` increase in MPG comparing to use of automatic transmission for the Model Initial.

Model Complete:

```{r confidenceIntervalModelComplete, echo=TRUE}
sumCoef.Complete <- summary(model.Complete)$coef
confInterval.Model.Complete <- sumCoef.Complete[1,1] + c(-1,1) * qt(0.975, df=model.Complete$df) * sumCoef.Complete[1,2]
```

> With 95% confidence, we estimate that a manual transmission results in a `r confInterval.Model.Complete[1]` to `r confInterval.Model.Complete[2]` increase in MPG comparing to use of automatic transmission for the Model Complete.

Model Proposed:

```{r confidenceIntervalModelStep, echo=TRUE}
sumCoef.Step <- summary(model.Step)$coef
confInterval.Model.Step <- sumCoef.Step[1,1] + c(-1,1) * qt(0.975, df=model.Step$df) * sumCoef.Step[1,2]
```

```{r confidenceIntervalModelProp, echo=TRUE}
sumCoef.Prop <- summary(model.Prop)$coef
confInterval.Model.Prop <- sumCoef.Prop[1,1] + c(-1,1) * qt(0.975, df=model.Prop$df) * sumCoef.Prop[1,2]
```

> With 95% confidence, we estimate that a manual transmission results in a `r confInterval.Model.Step[1]` to `r confInterval.Model.Step[2]` increase in MPG comparing to use of automatic transmission for the Model Step.

***


##Main Body + Apendix only figures (not more than 5)

Figure :
```{r pairsMtcars}
pairs(mtcars, panel = panel.smooth, main = "mtcars data", col=3+(mtcars$am>0))
```

Figure 2:

```{r plotModelInitial, echo=TRUE}
plot(x=mtcars$am, y=mtcars$mpg,col=mtcars$am+1)
legend("top",c("Automatic","Manual"),col=c(1,2),pch=1)
abline(model.Initial, col="blue")


with(mtcars, plot(x=wt + qsec + am, y=mtcars$mpg,col=mtcars$am+1))
legend("top",c("Automatic","Manual"),col=c(1,2),pch=1)

plot(cooks.distance(model.Complete), main="Cook's Distance for Mtcars")
dfbetasPlots(model.Complete, main="Plot of Mtcars DFBETAS")

#Dffits
dffitsData.Complete <- as.data.frame(dffits(model.Complete))
names(dffitsData.Complete) <- c("Observations")
cutoff <- 2*sqrt(11/length(mtcars$mpg))
plot(dffitsData.Complete$Observations, main="DFFITS Plot")
abline(h=cutoff)
abline(h=-cutoff)
labels=row.names(mtcars)

```

Figure 3 Plot of residuals:
```{r plotResidual, echo=TRUE}
plot(x, resid(model.Initial))
abline(h=0)
plot(x, resid(model.Complete))
abline(h=0)
plot(x, resid(model.Step))
abline(h=0)
```